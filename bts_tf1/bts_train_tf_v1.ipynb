{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BtsDataloader(object):\n",
    "    \"\"\"bts dataloader\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, gt_path, filenames_file, params, mode,\n",
    "                 do_rotate=False, degree=5.0, do_kb_crop=False):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.gt_path = gt_path\n",
    "        self.params = params\n",
    "        self.mode = mode\n",
    "\n",
    "        self.do_rotate = do_rotate\n",
    "        self.degree = degree\n",
    "\n",
    "        self.do_kb_crop = do_kb_crop\n",
    "\n",
    "        with open(filenames_file, 'r') as f:\n",
    "            filenames = f.readlines()\n",
    "\n",
    "        if mode == 'train':\n",
    "            assert not self.params.batch_size % self.params.num_gpus\n",
    "            mini_batch_size = int(self.params.batch_size / self.params.num_gpus)\n",
    "\n",
    "            self.loader = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "            self.loader = self.loader.apply(tf.contrib.data.shuffle_and_repeat(len(filenames)))\n",
    "            self.loader = self.loader.map(self.parse_function_train, num_parallel_calls=params.num_threads)\n",
    "            self.loader = self.loader.map(self.train_preprocess, num_parallel_calls=params.num_threads)\n",
    "            self.loader = self.loader.batch(mini_batch_size)\n",
    "            self.loader = self.loader.prefetch(mini_batch_size)\n",
    "\n",
    "        else:\n",
    "            self.loader = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "            self.loader = self.loader.map(self.parse_function_test, num_parallel_calls=1)\n",
    "            self.loader = self.loader.map(self.test_preprocess, num_parallel_calls=1)\n",
    "            self.loader = self.loader.batch(1)\n",
    "            self.loader = self.loader.prefetch(1)\n",
    "\n",
    "    def parse_function_test(self, line):\n",
    "        split_line = tf.string_split([line]).values\n",
    "        image_path = tf.string_join([self.data_path, split_line[0]])\n",
    "\n",
    "        if self.params.dataset == 'nyu':\n",
    "            image = tf.image.decode_jpeg(tf.read_file(image_path))\n",
    "        else:\n",
    "            image = tf.image.decode_png(tf.read_file(image_path))\n",
    "\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        focal = tf.string_to_number(split_line[2])\n",
    "\n",
    "        if self.do_kb_crop is True:\n",
    "            height = tf.shape(image)[0]\n",
    "            width = tf.shape(image)[1]\n",
    "            top_margin = tf.to_int32(height - 352)\n",
    "            left_margin = tf.to_int32((width - 1216) / 2)\n",
    "            image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n",
    "\n",
    "        return image, focal\n",
    "\n",
    "    def test_preprocess(self, image, focal):\n",
    "\n",
    "        image.set_shape([None, None, 3])\n",
    "        \n",
    "        image *= 255.0\n",
    "        image = self.mean_image_subtraction(image, [123.68, 116.78, 103.94])\n",
    "\n",
    "        if self.params.encoder == 'densenet161_bts' or self.params.encoder == 'densenet121_bts':\n",
    "            image *= 0.017\n",
    "\n",
    "        return image, focal\n",
    "\n",
    "    def parse_function_train(self, line):\n",
    "        split_line = tf.string_split([line]).values\n",
    "        image_path = tf.string_join([self.data_path, split_line[0]])\n",
    "        depth_gt_path = tf.string_join([self.gt_path, tf.string_strip(split_line[1])])\n",
    "\n",
    "        if self.params.dataset == 'nyu':\n",
    "            image = tf.image.decode_jpeg(tf.read_file(image_path))\n",
    "        else:\n",
    "            image = tf.image.decode_png(tf.read_file(image_path))\n",
    "\n",
    "        depth_gt = tf.image.decode_png(tf.read_file(depth_gt_path), channels=0, dtype=tf.uint16)\n",
    "\n",
    "        if self.params.dataset == 'nyu':\n",
    "            depth_gt = tf.cast(depth_gt, tf.float32) / 1000.0\n",
    "        else:\n",
    "            depth_gt = tf.cast(depth_gt, tf.float32) / 256.0\n",
    "\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        focal = tf.string_to_number(split_line[2])\n",
    "\n",
    "        # To avoid blank boundaries due to pixel registration\n",
    "        if self.params.dataset == 'nyu':\n",
    "            depth_gt = depth_gt[45:472, 43:608, :]\n",
    "            image = image[45:472, 43:608, :]\n",
    "\n",
    "        if self.do_kb_crop is True:\n",
    "            print('Cropping training images as kitti benchmark images')\n",
    "            height = tf.shape(image)[0]\n",
    "            width = tf.shape(image)[1]\n",
    "            top_margin = tf.to_int32(height - 352)\n",
    "            left_margin = tf.to_int32((width - 1216) / 2)\n",
    "            depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n",
    "            image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]\n",
    "\n",
    "        if self.do_rotate is True:\n",
    "            random_angle = tf.random_uniform([], - self.degree * 3.141592 / 180, self.degree * 3.141592 / 180)\n",
    "            image = tf.contrib.image.rotate(image, random_angle, interpolation='BILINEAR')\n",
    "            depth_gt = tf.contrib.image.rotate(depth_gt, random_angle, interpolation='NEAREST')\n",
    "\n",
    "        print('Do random cropping from fixed size input')\n",
    "        image, depth_gt = self.random_crop_fixed_size(image, depth_gt)\n",
    "\n",
    "        return image, depth_gt, focal\n",
    "\n",
    "    def train_preprocess(self, image, depth_gt, focal):\n",
    "        # Random flipping\n",
    "        do_flip = tf.random_uniform([], 0, 1)\n",
    "        image = tf.cond(do_flip > 0.5, lambda: tf.image.flip_left_right(image), lambda: image)\n",
    "        depth_gt = tf.cond(do_flip > 0.5, lambda: tf.image.flip_left_right(depth_gt), lambda: depth_gt)\n",
    "\n",
    "        # Random gamma, brightness, color augmentation\n",
    "        do_augment = tf.random_uniform([], 0, 1)\n",
    "        image = tf.cond(do_augment > 0.5, lambda: self.augment_image(image), lambda: image)\n",
    "\n",
    "        image.set_shape([self.params.height, self.params.width, 3])\n",
    "        depth_gt.set_shape([self.params.height, self.params.width, 1])\n",
    "        \n",
    "        image *= 255.0\n",
    "        image = self.mean_image_subtraction(image, [123.68, 116.78, 103.94])\n",
    "\n",
    "        if self.params.encoder == 'densenet161_bts' or self.params.encoder == 'densenet121_bts':\n",
    "            image *= 0.017\n",
    "        \n",
    "        return image, depth_gt, focal\n",
    "\n",
    "    def random_crop_fixed_size(self, image, depth_gt):\n",
    "        image_depth = tf.concat([image, depth_gt], 2)\n",
    "        image_depth_cropped = tf.random_crop(image_depth, [self.params.height, self.params.width, 4])\n",
    "\n",
    "        image_cropped = image_depth_cropped[:, :, 0:3]\n",
    "        depth_gt_cropped = tf.expand_dims(image_depth_cropped[:, :, 3], 2)\n",
    "\n",
    "        return image_cropped, depth_gt_cropped\n",
    "\n",
    "    def augment_image(self, image):\n",
    "        # gamma augmentation\n",
    "        gamma = tf.random_uniform([], 0.9, 1.1)\n",
    "        image_aug = image ** gamma\n",
    "\n",
    "        # brightness augmentation\n",
    "        if self.params.dataset == 'nyu':\n",
    "            brightness = tf.random_uniform([], 0.75, 1.25)\n",
    "        else:\n",
    "            brightness = tf.random_uniform([], 0.9, 1.1)\n",
    "        image_aug = image_aug * brightness\n",
    "\n",
    "        # color augmentation\n",
    "        colors = tf.random_uniform([3], 0.9, 1.1)\n",
    "        white = tf.ones([tf.shape(image)[0], tf.shape(image)[1]])\n",
    "        color_image = tf.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        image_aug *= color_image\n",
    "\n",
    "        # clip\n",
    "        if self.params.encoder == 'densenet161_bts' or self.params.encoder == 'densenet121_bts':\n",
    "            image_aug = tf.clip_by_value(image_aug,  0, 1)\n",
    "        else:\n",
    "            image_aug = tf.clip_by_value(image_aug, 0, 255)\n",
    "\n",
    "        return image_aug\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_image_subtraction(image, means):\n",
    "        \"\"\"Subtracts the given means from each image channel.\n",
    "        For example:\n",
    "          means = [123.68, 116.779, 103.939]\n",
    "          image = mean_image_subtraction(image, means)\n",
    "        Note that the rank of `image` must be known.\n",
    "        Args:\n",
    "          image: a tensor of size [height, width, C].\n",
    "          means: a C-vector of values to subtract from each channel.\n",
    "        Returns:\n",
    "          the centered image.\n",
    "        Raises:\n",
    "          ValueError: If the rank of `image` is unknown, if `image` has a rank other\n",
    "            than three or if the number of channels in `image` doesn't match the\n",
    "            number of values in `means`.\n",
    "        \"\"\"\n",
    "\n",
    "        if image.get_shape().ndims != 3:\n",
    "            raise ValueError('Input must be of size [height, width, C>0]')\n",
    "        num_channels = image.get_shape().as_list()[-1]\n",
    "        if len(means) != num_channels:\n",
    "            raise ValueError('len(means) must match the number of channels')\n",
    "\n",
    "        channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n",
    "        for i in range(num_channels):\n",
    "            channels[i] -= means[i]\n",
    "        return tf.concat(axis=2, values=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arg_line_to_args(arg_line):\n",
    "    for arg in arg_line.split():\n",
    "        if not arg.strip():\n",
    "            continue\n",
    "        yield arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='train'\n",
    "model_name='bts_nyu_test'\n",
    "encoder='densenet161_bts'\n",
    "dataset='nyu'\n",
    "data_path='../dataset/nyu_depth_v2/sync/'\n",
    "gt_path ='../dataset/nyu_depth_v2/sync/'\n",
    "filenames_file ='../train_test_inputs/nyudepthv2_train_files_with_gt_small.txt'\n",
    "batch_size=1\n",
    "num_epochs=1\n",
    "learning_rate=1e-4\n",
    "end_learning_rate=-1\n",
    "num_gpus=1\n",
    "num_threads=1\n",
    "input_height=416\n",
    "input_width=544\n",
    "max_depth=10\n",
    "do_random_rotate=True\n",
    "degree=2.5\n",
    "log_directory ='./models/'\n",
    "pretrained_model= './models/densenet161_imagenet/model'\n",
    "fix_first_conv_blocks = True\n",
    "checkpoint_path =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./custom_layer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _local_planar_guidance_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpg = tf.load_op_library('custom_layer/build/liblpg.so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts_parameters = namedtuple('parameters', 'encoder, '\n",
    "                                          'height, width, '\n",
    "                                          'max_depth, '\n",
    "                                          'batch_size, '\n",
    "                                          'dataset, '\n",
    "                                          'num_gpus, '\n",
    "                                          'num_threads, '\n",
    "                                          'num_epochs, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BtsModel(object):\n",
    "\n",
    "    def __init__(self, params, mode, image, depth_gt, focal, reuse_variables=None, model_index=0, bn_training=False):\n",
    "        self.params = params\n",
    "        self.mode = mode\n",
    "        self.max_depth = self.params.max_depth\n",
    "\n",
    "        self.input_image = image\n",
    "        self.depth_gt = depth_gt\n",
    "        self.focal = tf.cast(focal, tf.float32)\n",
    "        self.model_collection = ['model_' + str(model_index)]\n",
    "\n",
    "        self.reuse_variables = reuse_variables\n",
    "        self.bn_training = bn_training\n",
    "        self.is_training = True if mode == 'train' else False\n",
    "\n",
    "        self.build_model(net_input=self.input_image, reuse=self.reuse_variables)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return\n",
    "\n",
    "        self.build_losses()\n",
    "        self.build_summaries()\n",
    "\n",
    "    def upsample_nn(self, x, ratio):\n",
    "        s = tf.shape(x)\n",
    "        h = s[1]\n",
    "        w = s[2]\n",
    "        return tf.image.resize_nearest_neighbor(x, [h * ratio, w * ratio], align_corners=True)\n",
    "    \n",
    "    def downsample_nn(self, x, ratio):\n",
    "        s = tf.shape(x)\n",
    "        h = tf.cast(s[1] / ratio, tf.int32)\n",
    "        w = tf.cast(s[2] / ratio, tf.int32)\n",
    "        return tf.image.resize_nearest_neighbor(x, [h, w], align_corners=True)\n",
    "    \n",
    "    def conv(self, x, num_out_layers, kernel_size, stride, activation_fn=tf.nn.elu, normalizer_fn=None):\n",
    "        p = np.floor((kernel_size - 1) / 2).astype(np.int32)\n",
    "        p_x = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]])\n",
    "        return slim.conv2d(p_x, num_out_layers, kernel_size, stride, 'VALID', activation_fn=activation_fn, normalizer_fn=normalizer_fn)\n",
    "\n",
    "    def atrous_conv(self, x, num_out_layers, kernel_size, rate, apply_bn_first=True):\n",
    "        pk = np.floor((kernel_size - 1) / 2).astype(np.int32)\n",
    "        pr = rate - 1\n",
    "        p = pk + pr\n",
    "        out = tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]])\n",
    "\n",
    "        if apply_bn_first is True:\n",
    "            out = slim.batch_norm(out)\n",
    "\n",
    "        out = tf.nn.relu(out)\n",
    "        out = slim.conv2d(out, num_out_layers * 2, 1, 1, 'VALID')\n",
    "        out = slim.batch_norm(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = slim.conv2d(out, num_out_layers, kernel_size=kernel_size, stride=1, rate=rate, padding='VALID',\n",
    "                          activation_fn=None, normalizer_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def upconv(self, x, num_out_layers, kernel_size, scale, activation_fn=tf.nn.elu, normalizer_fn=None):\n",
    "        upsample = self.upsample_nn(x, scale)\n",
    "        conv = self.conv(upsample, num_out_layers, kernel_size, 1, activation_fn=activation_fn, normalizer_fn=normalizer_fn)\n",
    "        return conv\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def denseconv(self, x, num_filters, kernel_size, stride=1, dilation_rate=1, dropout_rate=None, scope=None):\n",
    "        with tf.variable_scope(scope, 'xx', [x]) as sc:\n",
    "            out = slim.batch_norm(x, is_training=False)\n",
    "            out = tf.nn.relu(out)\n",
    "            out = slim.conv2d(out, num_filters, kernel_size, rate=dilation_rate, activation_fn=None)\n",
    "            if dropout_rate:\n",
    "                out = tf.nn.dropout(out)\n",
    "            return out\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def denseconv_block(self, x, num_filters, dilation_rate=1, scope=None):\n",
    "        with tf.variable_scope(scope, 'conv_blockx', [x]) as sc:\n",
    "            out = self.denseconv(x, num_filters * 4, 1, scope='x1')\n",
    "            out = self.denseconv(out, num_filters, 3, dilation_rate=dilation_rate, scope='x2')\n",
    "            out = tf.concat([x, out], axis=3)\n",
    "            return out\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def dense_block(self, x, num_layers, num_filters, growth_rate, dilation_rate=1, grow_num_filters=True, scope=None):\n",
    "        with tf.variable_scope(scope, 'dense_blockx', [x]) as sc:\n",
    "            out = x\n",
    "            for i in range(num_layers):\n",
    "                branch = i + 1\n",
    "                out = self.denseconv_block(out, growth_rate, dilation_rate=dilation_rate,\n",
    "                                           scope='conv_block' + str(branch))\n",
    "                if grow_num_filters:\n",
    "                    num_filters += growth_rate\n",
    "            return out, num_filters\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def transition_block(self, x, num_filters, compression=1.0, do_pooling=True, scope=None):\n",
    "        num_filters = int(num_filters * compression)\n",
    "        with tf.variable_scope(scope, 'transition_blockx', [x]) as sc:\n",
    "            out = self.denseconv(x, num_filters, 1, scope='blk')\n",
    "            if do_pooling:\n",
    "                out = slim.avg_pool2d(out, 2)\n",
    "            return out, num_filters\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def reduction_1x1(self, net, num_filters, is_final=False):\n",
    "        while num_filters >= 4:\n",
    "            if num_filters < 8:\n",
    "                if is_final:\n",
    "                    net = self.conv(net, 1, 1, 1, activation_fn=tf.nn.sigmoid)\n",
    "                else:\n",
    "                    net = self.conv(net, 3, 1, 1, activation_fn=None)\n",
    "                    theta = tf.nn.sigmoid(net[:, :, :, 0]) * 3.1415926535 / 6\n",
    "                    phi = tf.nn.sigmoid(net[:, :, :, 1]) * 3.1415926535 * 2\n",
    "                    dist = tf.nn.sigmoid(net[:, :, :, 2]) * self.max_depth\n",
    "                    n1 = tf.expand_dims(tf.multiply(tf.math.sin(theta), tf.math.cos(phi)), 3)\n",
    "                    n2 = tf.expand_dims(tf.multiply(tf.math.sin(theta), tf.math.sin(phi)), 3)\n",
    "                    n3 = tf.expand_dims(tf.math.cos(theta), 3)\n",
    "                    n4 = tf.expand_dims(dist, 3)\n",
    "                    net = tf.concat([n1, n2, n3, n4], axis=3)\n",
    "                break\n",
    "            else:\n",
    "                net = self.conv(net, num_filters, 1, 1)\n",
    "\n",
    "            num_filters = num_filters / 2\n",
    "\n",
    "        return net\n",
    "\n",
    "    def get_depth(self, x):\n",
    "        depth = self.max_depth * self.conv(x, 1, 3, 1, tf.nn.sigmoid, normalizer_fn=None)\n",
    "        if self.params.dataset == 'kitti':\n",
    "            focal_expanded = tf.expand_dims(self.focal, 1)\n",
    "            focal_expanded = tf.expand_dims(focal_expanded, 1)\n",
    "            focal_expanded = tf.expand_dims(focal_expanded, 1)\n",
    "            depth = depth * focal_expanded / 715.0873 # Average focal length in KITTI Eigen training set\n",
    "        return depth\n",
    "    \n",
    "    def densenet(self, inputs, reduction=None, growth_rate=None, num_filters=None, num_layers=None, dropout_rate=None,\n",
    "                 is_training=True, reuse=None, scope=None):\n",
    "\n",
    "        assert reduction is not None\n",
    "        assert growth_rate is not None\n",
    "        assert num_filters is not None\n",
    "        assert num_layers is not None\n",
    "\n",
    "        compression = 1.0 - reduction\n",
    "        num_dense_blocks = len(num_layers)\n",
    "\n",
    "        batch_norm_params = {'is_training': False,\n",
    "                             'scale': True,\n",
    "                             'decay': 0.99,\n",
    "                             'epsilon': 1.1e-5,\n",
    "                             'fused': True, }\n",
    "\n",
    "        with tf.variable_scope(scope, 'densenetxxx', [inputs], reuse=reuse) as sc:\n",
    "            with slim.arg_scope([slim.dropout], is_training=is_training),\\\n",
    "                 slim.arg_scope([slim.batch_norm], **batch_norm_params),\\\n",
    "                 slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(1e-4), activation_fn=None, biases_initializer=None):\n",
    "\n",
    "                skips = []\n",
    "\n",
    "                net = inputs\n",
    "\n",
    "                # Initial convolution\n",
    "                net = slim.conv2d(net, num_filters, 7, stride=2, scope='conv1')  # H/2\n",
    "                net = slim.batch_norm(net, is_training=False)\n",
    "                net = tf.nn.relu(net)\n",
    "\n",
    "                skips.append(net)\n",
    "\n",
    "                net = slim.max_pool2d(net, 3, stride=2, padding='SAME')  # H/4\n",
    "                skips.append(net)\n",
    "\n",
    "                # Blocks\n",
    "                for i in range(num_dense_blocks - 1):  # i:0 H/8, i:1 H/16, i:2 H/32\n",
    "                    do_pooling = True\n",
    "                    dilation_rate = 1\n",
    "\n",
    "                    net, num_filters = self.dense_block(net, num_layers[i], num_filters, growth_rate,\n",
    "                                                        dilation_rate=dilation_rate, scope='dense_block' + str(i + 1))\n",
    "\n",
    "                    # Add transition_block\n",
    "                    net, num_filters = self.transition_block(net, num_filters, compression=compression,\n",
    "                                                             do_pooling=do_pooling,\n",
    "                                                             scope='transition_block' + str(i + 1))\n",
    "                    if i < num_dense_blocks - 2:\n",
    "                        skips.append(net)\n",
    "\n",
    "                net, num_filters = self.dense_block(net, num_layers[-1], num_filters, growth_rate,\n",
    "                                                    scope='dense_block' + str(num_dense_blocks))\n",
    "\n",
    "                with tf.variable_scope('final_block', [inputs]):\n",
    "                    net = slim.batch_norm(net, is_training=False)\n",
    "                    net = tf.nn.relu(net)\n",
    "\n",
    "                return net, skips\n",
    "\n",
    "    @slim.add_arg_scope\n",
    "    def bts(self, dense_features, skips, num_filters=256):\n",
    "        batch_norm_params = {'is_training': self.bn_training,\n",
    "                             'scale': True,\n",
    "                             'decay': 0.99,\n",
    "                             'epsilon': 1.1e-5,\n",
    "                             'fused': True, }\n",
    "\n",
    "        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "            conv = self.conv\n",
    "            atrous_conv = self.atrous_conv\n",
    "            upconv = self.upconv\n",
    "\n",
    "            upconv5 = upconv(dense_features, num_filters, 3, 2)  # H/16\n",
    "            upconv5 = slim.batch_norm(upconv5)\n",
    "            concat5 = tf.concat([upconv5, skips[3]], 3)\n",
    "            iconv5 = conv(concat5, num_filters, 3, 1)\n",
    "\n",
    "            num_filters = num_filters / 2\n",
    "\n",
    "            upconv4 = upconv(iconv5, num_filters, 3, 2)  # H/8\n",
    "            upconv4 = slim.batch_norm(upconv4)\n",
    "            concat4 = tf.concat([upconv4, skips[2]], 3)\n",
    "            iconv4 = conv(concat4, num_filters, 3, 1)\n",
    "            iconv4 = slim.batch_norm(iconv4)\n",
    "\n",
    "            daspp_3 = atrous_conv(iconv4, num_filters / 2, kernel_size=3, rate=3, apply_bn_first=False)\n",
    "            concat4_2 = tf.concat([concat4, daspp_3], 3)\n",
    "            daspp_6 = atrous_conv(concat4_2, num_filters / 2, kernel_size=3, rate=6)\n",
    "            concat4_3 = tf.concat([concat4_2, daspp_6], 3)\n",
    "            daspp_12 = atrous_conv(concat4_3, num_filters / 2, kernel_size=3, rate=12)\n",
    "            concat4_4 = tf.concat([concat4_3, daspp_12], 3)\n",
    "            daspp_18 = atrous_conv(concat4_4, num_filters / 2, kernel_size=3, rate=18)\n",
    "            concat4_5 = tf.concat([concat4_4, daspp_18], 3)\n",
    "            daspp_24 = atrous_conv(concat4_5, num_filters / 2, kernel_size=3, rate=24)\n",
    "            concat4_daspp = tf.concat([iconv4, daspp_3, daspp_6, daspp_12, daspp_18, daspp_24], 3)\n",
    "            daspp_feat = conv(concat4_daspp, num_filters / 2, 3, 1)\n",
    "\n",
    "            plane_eq_8x8 = self.reduction_1x1(daspp_feat, num_filters / 2)\n",
    "            plane_normal_8x8 = tf.nn.l2_normalize(plane_eq_8x8[:, :, :, 0:3], axis=3)\n",
    "            plane_dist_8x8 = plane_eq_8x8[:, :, :, 3]\n",
    "            plane_eq_8x8 = tf.concat([plane_normal_8x8, tf.expand_dims(plane_dist_8x8, 3)], 3)\n",
    "            depth_8x8 = lpg.local_planar_guidance(plane_eq_8x8, upratio=8, focal=self.focal)\n",
    "            depth_8x8_scaled = tf.expand_dims(depth_8x8, 3) / self.max_depth\n",
    "            depth_8x8_scaled_ds = self.downsample_nn(depth_8x8_scaled, 4)\n",
    "\n",
    "            num_filters = num_filters / 2\n",
    "\n",
    "            upconv3 = upconv(daspp_feat, num_filters, 3, 2)  # H/4\n",
    "            upconv3 = slim.batch_norm(upconv3)\n",
    "            concat3 = tf.concat([upconv3, skips[1], depth_8x8_scaled_ds], 3)\n",
    "            iconv3 = conv(concat3, num_filters, 3, 1)\n",
    "\n",
    "            plane_eq_4x4 = self.reduction_1x1(iconv3, num_filters / 2)\n",
    "            plane_normal_4x4 = tf.nn.l2_normalize(plane_eq_4x4[:, :, :, 0:3], axis=3)\n",
    "            plane_dist_4x4 = plane_eq_4x4[:, :, :, 3]\n",
    "            plane_eq_4x4 = tf.concat([plane_normal_4x4, tf.expand_dims(plane_dist_4x4, 3)], 3)\n",
    "            depth_4x4 = lpg.local_planar_guidance(plane_eq_4x4, upratio=4, focal=self.focal)\n",
    "            depth_4x4_scaled = tf.expand_dims(depth_4x4, 3) / self.max_depth\n",
    "            depth_4x4_scaled_ds = self.downsample_nn(depth_4x4_scaled, 2)\n",
    "\n",
    "            num_filters = num_filters / 2\n",
    "\n",
    "            upconv2 = upconv(iconv3, num_filters, 3, 2)  # H/2\n",
    "            upconv2 = slim.batch_norm(upconv2)\n",
    "            concat2 = tf.concat([upconv2, skips[0], depth_4x4_scaled_ds], 3)\n",
    "            iconv2 = conv(concat2, num_filters, 3, 1)\n",
    "\n",
    "            plane_eq_2x2 = self.reduction_1x1(iconv2, num_filters / 2)\n",
    "            plane_normal_2x2 = tf.nn.l2_normalize(plane_eq_2x2[:, :, :, 0:3], axis=3)\n",
    "            plane_dist_2x2 = plane_eq_2x2[:, :, :, 3]\n",
    "            plane_eq_2x2 = tf.concat([plane_normal_2x2, tf.expand_dims(plane_dist_2x2, 3)], 3)\n",
    "            depth_2x2 = lpg.local_planar_guidance(plane_eq_2x2, upratio=2, focal=self.focal)\n",
    "            depth_2x2_scaled = tf.expand_dims(depth_2x2, 3) / self.max_depth\n",
    "\n",
    "            num_filters = num_filters / 2\n",
    "\n",
    "            upconv1 = upconv(iconv2, num_filters, 3, 2)  # H\n",
    "            reduc1x1 = self.reduction_1x1(upconv1, num_filters, is_final=True)\n",
    "            concat1 = tf.concat([upconv1, reduc1x1, depth_2x2_scaled, depth_4x4_scaled, depth_8x8_scaled], 3)\n",
    "            iconv1 = conv(concat1, num_filters, 3, 1)\n",
    "\n",
    "            self.depth_est = self.get_depth(iconv1)\n",
    "            self.lpg2x2 = depth_2x2_scaled\n",
    "            self.lpg4x4 = depth_4x4_scaled\n",
    "            self.lpg8x8 = depth_8x8_scaled\n",
    "            self.reduc1x1 = reduc1x1\n",
    "\n",
    "            print(\"==================================\")\n",
    "            print(\" upconv5 in/out: {} / {}\".format(dense_features.shape[-1], upconv5.shape[-1]))\n",
    "            print(\"  iconv5 in/out: {} / {}\".format(concat5.shape[-1], iconv5.shape[-1]))\n",
    "            print(\" upconv4 in/out: {} / {}\".format(iconv5.shape[-1], upconv4.shape[-1]))\n",
    "            print(\"  iconv4 in/out: {} / {}\".format(concat4.shape[-1], iconv4.shape[-1]))\n",
    "            print(\"    aspp in/out: {} / {}\".format(concat4_daspp.shape[-1], daspp_feat.shape[-1]))\n",
    "            print(\"reduc8x8 in/out: {} / {}\".format(daspp_feat.shape[-1], plane_eq_8x8.shape[-1]))\n",
    "            print(\"  lpg8x8 in/out: {} / {}\".format(plane_eq_8x8.shape[-1], 1))\n",
    "            print(\" upconv3 in/out: {} / {}\".format(daspp_feat.shape[-1], upconv3.shape[-1]))\n",
    "            print(\"  iconv3 in/out: {} / {}\".format(concat3.shape[-1], iconv3.shape[-1]))\n",
    "            print(\"reduc4x4 in/out: {} / {}\".format(iconv3.shape[-1], plane_eq_4x4.shape[-1]))\n",
    "            print(\"  lpg4x4 in/out: {} / {}\".format(plane_eq_4x4.shape[-1], 1))\n",
    "            print(\" upconv2 in/out: {} / {}\".format(iconv3.shape[-1], upconv2.shape[-1]))\n",
    "            print(\"  iconv2 in/out: {} / {}\".format(concat2.shape[-1], iconv2.shape[-1]))\n",
    "            print(\"reduc2x2 in/out: {} / {}\".format(iconv2.shape[-1], plane_eq_2x2.shape[-1]))\n",
    "            print(\"  lpg2x2 in/out: {} / {}\".format(plane_eq_2x2.shape[-1], 1))\n",
    "            print(\" upconv1 in/out: {} / {}\".format(iconv2.shape[-1], upconv1.shape[-1]))\n",
    "            print(\"reduc1x1 in/out: {} / {}\".format(upconv1.shape[-1], reduc1x1.shape[-1]))\n",
    "            print(\"  iconv1 in/out: {} / {}\".format(concat1.shape[-1], iconv1.shape[-1]))\n",
    "            print(\"   depth in/out: {} / {}\".format(iconv1.shape[-1], self.depth_est.shape[-1]))\n",
    "            print(\"==================================\")\n",
    "\n",
    "    def build_resnet101_bts(self, net_input, reuse):\n",
    "        batch_norm_params = {\n",
    "            'is_training': False,\n",
    "            'decay': 0.997,\n",
    "            'epsilon': 1e-5,\n",
    "            'scale': True,\n",
    "            'fused': True,  # Use fused batch norm if possible.\n",
    "        }\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with slim.arg_scope([slim.conv2d],\n",
    "                                weights_regularizer=slim.l2_regularizer(1e-4),\n",
    "                                weights_initializer=slim.variance_scaling_initializer(),\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=slim.batch_norm,\n",
    "                                normalizer_params=batch_norm_params),\\\n",
    "                 slim.arg_scope([slim.batch_norm], **batch_norm_params),\\\n",
    "                 slim.arg_scope([slim.max_pool2d], padding='SAME'):\n",
    "\n",
    "                dense_features, skips, endpoints = resnet_v1_101(net_input, global_pool=False, spatial_squeeze=False,\n",
    "                                                                 is_training=self.is_training, reuse=reuse, scope='resnet101')\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=tf.nn.elu):\n",
    "                self.bts(dense_features, skips, num_filters=512)\n",
    "\n",
    "    def build_resnet50_bts(self, net_input, reuse):\n",
    "        batch_norm_params = {\n",
    "            'is_training': False,\n",
    "            'decay': 0.997,\n",
    "            'epsilon': 1e-5,\n",
    "            'scale': True,\n",
    "            'fused': True,  # Use fused batch norm if possible.\n",
    "        }\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with slim.arg_scope([slim.conv2d],\n",
    "                                weights_regularizer=slim.l2_regularizer(1e-4),\n",
    "                                weights_initializer=slim.variance_scaling_initializer(),\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                normalizer_fn=slim.batch_norm,\n",
    "                                normalizer_params=batch_norm_params), \\\n",
    "                 slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n",
    "                 slim.arg_scope([slim.max_pool2d], padding='SAME'):\n",
    "\n",
    "                dense_features, skips, endpoints = resnet_v1_50(net_input, global_pool=False, spatial_squeeze=False,\n",
    "                                                                is_training=self.is_training, reuse=reuse, scope='resnet50')\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=tf.nn.elu):\n",
    "                self.bts(dense_features, skips, num_filters=256)\n",
    "\n",
    "    def build_densenet121_bts(self, net_input, reuse):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            dense_features, skips = self.densenet(net_input, reduction=0.5, growth_rate=32,\n",
    "                                                  num_filters=self.num_filters, num_layers=[6, 12, 24, 16],\n",
    "                                                  is_training=self.is_training, reuse=reuse, scope='densenet121')\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=tf.nn.elu):\n",
    "                self.bts(dense_features, skips, num_filters=256)\n",
    "\n",
    "    def build_densenet161_bts(self, net_input, reuse):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            dense_features, skips = self.densenet(net_input, reduction=0.5, growth_rate=48,\n",
    "                                                  num_filters=self.num_filters, num_layers=[6, 12, 36, 24],\n",
    "                                                  is_training=self.is_training, reuse=reuse, scope='densenet161')\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=tf.nn.elu):\n",
    "                self.bts(dense_features, skips, num_filters=512)\n",
    "\n",
    "    def build_model(self, net_input, reuse):\n",
    "        with tf.variable_scope('model', reuse=reuse):\n",
    "            if self.params.encoder == 'densenet161_bts':\n",
    "                self.num_filters = 96\n",
    "                self.build_densenet161_bts(net_input=net_input, reuse=reuse)\n",
    "            elif self.params.encoder == 'densenet121_bts':\n",
    "                self.num_filters = 64\n",
    "                self.build_densenet121_bts(net_input=net_input, reuse=reuse)\n",
    "            elif self.params.encoder == 'resnet101_bts':\n",
    "                self.build_resnet101_bts(net_input=net_input, reuse=reuse)\n",
    "            elif self.params.encoder == 'resnet50_bts':\n",
    "                self.build_resnet50_bts(net_input=net_input, reuse=reuse)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def build_losses(self):\n",
    "        with tf.variable_scope('losses', reuse=self.reuse_variables):\n",
    "\n",
    "            if self.params.dataset == 'nyu':\n",
    "                self.mask = self.depth_gt > 0.1\n",
    "            else:\n",
    "                self.mask = self.depth_gt > 1.0\n",
    "\n",
    "            depth_gt_masked = tf.boolean_mask(self.depth_gt, self.mask)\n",
    "            depth_est_masked = tf.boolean_mask(self.depth_est, self.mask)\n",
    "\n",
    "            d = tf.log(depth_est_masked) - tf.log(depth_gt_masked)  # Best\n",
    "\n",
    "            self.silog_loss = tf.sqrt(tf.reduce_mean(d ** 2) - 0.85 * (tf.reduce_mean(d) ** 2)) * 10.0\n",
    "            self.total_loss = self.silog_loss\n",
    "\n",
    "    def build_summaries(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            tf.summary.scalar('silog_loss', self.silog_loss, collections=self.model_collection)\n",
    "            depth_gt = tf.where(self.depth_gt < 1e-3, self.depth_gt * 0 + 1e3, self.depth_gt)\n",
    "            tf.summary.image('depth_gt', 1 / depth_gt, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('depth_est', 1 / self.depth_est, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('reduc1x1', 1 / self.reduc1x1, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('lpg2x2', 1 / self.lpg2x2, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('lpg4x4', 1 / self.lpg4x4, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('lpg8x8', 1 / self.lpg8x8, max_outputs=4, collections=self.model_collection)\n",
    "            tf.summary.image('image', self.input_image, max_outputs=4, collections=self.model_collection)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_lines(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    return len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors_in_checkpoint_file(file_name, all_tensors=True, tensor_name=None):\n",
    "    varlist = []\n",
    "    var_value = []\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n",
    "    if all_tensors:\n",
    "      var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "      for key in sorted(var_to_shape_map):\n",
    "        varlist.append(key)\n",
    "        var_value.append(reader.get_tensor(key))\n",
    "    else:\n",
    "        varlist.append(tensor_name)\n",
    "        var_value.append(reader.get_tensor(tensor_name))\n",
    "    return (varlist, var_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensors_in_checkpoint_file(loaded_tensors):\n",
    "    full_var_list = list()\n",
    "    var_check = set()\n",
    "    # Loop all loaded tensors\n",
    "    for i, tensor_name in enumerate(loaded_tensors[0]):\n",
    "        # Extract tensor\n",
    "        try:\n",
    "            tensor_aux = tf.get_default_graph().get_tensor_by_name(tensor_name+\":0\")\n",
    "        except:\n",
    "            print(tensor_name + ' is in pretrained model but not in current training model')\n",
    "        if tensor_aux not in var_check:\n",
    "            full_var_list.append(tensor_aux)\n",
    "            var_check.add(tensor_aux)\n",
    "    return full_var_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = model_name + '.py'\n",
    "command = 'mkdir ' + log_directory + '/' + model_name\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer_path = log_directory + '/' + model_name + '/' + 'custom_layer'\n",
    "command = 'mkdir ' + custom_layer_path\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = 'cp ' + './custom_layer/* ' + custom_layer_path + '/'\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py',\n",
       " '-f',\n",
       " '/root/.local/share/jupyter/runtime/kernel-bac5534f-ba0c-4753-bb17-567bd9a23109.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_out_path = log_directory + '/' + model_name + '/' + sys.argv[1]\n",
    "command = 'cp ' + sys.argv[1] + ' ' + args_out_path\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out_path = log_directory + '/' + model_name + '/' + model_filename\n",
    "command = 'cp bts.py ' + model_out_path\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "num_training_samples = get_num_lines(filenames_file)\n",
    "steps_per_epoch = np.ceil(num_training_samples / batch_size).astype(np.int32)\n",
    "num_total_steps = num_epochs * steps_per_epoch\n",
    "print(num_training_samples,steps_per_epoch,num_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Total number of samples: 100\n",
      "Total number of steps: 100\n"
     ]
    }
   ],
   "source": [
    "start_learning_rate = learning_rate\n",
    "end_learning_rate = start_learning_rate * 0.1\n",
    "learning_rate = tf.train.polynomial_decay(start_learning_rate, global_step, num_total_steps, end_learning_rate, 0.9)\n",
    "opt_step = tf.train.AdamOptimizer(learning_rate, epsilon=1e-8)\n",
    "\n",
    "print(\"Total number of samples: {}\".format(num_training_samples))\n",
    "print(\"Total number of steps: {}\".format(num_total_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing first two conv blocks\n"
     ]
    }
   ],
   "source": [
    "if fix_first_conv_blocks or fix_first_conv_block:\n",
    "    if fix_first_conv_blocks:\n",
    "        print('Fixing first two conv blocks')\n",
    "    else:\n",
    "        print('Fixing first conv block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = bts_parameters(\n",
    "        encoder=encoder,\n",
    "        height=input_height,\n",
    "        width=input_width,\n",
    "        batch_size=batch_size,\n",
    "        dataset=dataset,\n",
    "        max_depth=max_depth,\n",
    "        num_gpus=num_gpus,\n",
    "        num_threads=num_threads,\n",
    "        num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-86052c3afc41>:25: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "Do random cropping from fixed size input\n"
     ]
    }
   ],
   "source": [
    "dataloader = BtsDataloader(data_path, gt_path, filenames_file,params, mode,\n",
    "                           do_rotate=do_random_rotate, degree=degree,\n",
    "                           do_kb_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iter = dataloader.loader.make_initializable_iterator()\n",
    "iter_init_op = dataloader_iter.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      " upconv5 in/out: 2208 / 512\n",
      "  iconv5 in/out: 896 / 512\n",
      " upconv4 in/out: 512 / 256\n",
      "  iconv4 in/out: 448 / 256\n",
      "    aspp in/out: 896 / 128\n",
      "reduc8x8 in/out: 128 / 4\n",
      "  lpg8x8 in/out: 4 / 1\n",
      " upconv3 in/out: 128 / 128\n",
      "  iconv3 in/out: 225 / 128\n",
      "reduc4x4 in/out: 128 / 4\n",
      "  lpg4x4 in/out: 4 / 1\n",
      " upconv2 in/out: 128 / 64\n",
      "  iconv2 in/out: 161 / 64\n",
      "reduc2x2 in/out: 64 / 4\n",
      "  lpg2x2 in/out: 4 / 1\n",
      " upconv1 in/out: 64 / 32\n",
      "reduc1x1 in/out: 32 / 1\n",
      "  iconv1 in/out: 36 / 32\n",
      "   depth in/out: 32 / 1\n",
      "==================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tower_grads = []\n",
    "tower_losses = []\n",
    "reuse_variables = None\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            image, depth_gt, focal = dataloader_iter.get_next()\n",
    "            model = BtsModel(params, mode, image, depth_gt, focal=focal,\n",
    "                             reuse_variables=reuse_variables, model_index=i, bn_training=False)\n",
    "\n",
    "            loss = model.total_loss\n",
    "            tower_losses.append(loss)\n",
    "\n",
    "            reuse_variables = True\n",
    "\n",
    "            if fix_first_conv_blocks or fix_first_conv_block:\n",
    "                trainable_vars = tf.trainable_variables()\n",
    "                if encoder == 'resnet101_bts' or encoder == 'resnet50_bts':\n",
    "                    first_conv_name = encoder.replace('_bts', '') + '/conv1'\n",
    "                    if fix_first_conv_blocks:\n",
    "                        g_vars = [var for var in\n",
    "                                  trainable_vars if (first_conv_name or 'block1' or 'block2') not in var.name]\n",
    "                    else:\n",
    "                        g_vars = [var for var in\n",
    "                                  trainable_vars if (first_conv_name or 'block1') not in var.name]\n",
    "                else:\n",
    "                    if fix_first_conv_blocks:\n",
    "                        g_vars = [var for var in\n",
    "                                  trainable_vars if ('conv1' or 'dense_block1' or 'dense_block2' or 'transition_block1' or 'transition_block2') not in var.name]\n",
    "                    else:\n",
    "                        g_vars = [var for var in\n",
    "                                  trainable_vars if ('dense_block1' or 'transition_block1') not in var.name]\n",
    "            else:\n",
    "                g_vars = None\n",
    "\n",
    "            grads = opt_step.compute_gradients(loss, var_list=g_vars)\n",
    "\n",
    "            tower_grads.append(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    with tf.device('/gpu:%d' % (num_gpus - 1)):\n",
    "        grads = average_gradients(tower_grads)\n",
    "        apply_gradient_op = opt_step.apply_gradients(grads, global_step=global_step)\n",
    "        total_loss = tf.reduce_mean(tower_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('learning_rate', learning_rate, ['model_0'])\n",
    "tf.summary.scalar('total_loss', total_loss, ['model_0'])\n",
    "summary_op = tf.summary.merge_all('model_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 47006235\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(log_directory + '/' + model_name, sess.graph)\n",
    "train_saver = tf.train.Saver(max_to_keep=200)\n",
    "\n",
    "total_num_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    total_num_parameters += np.array(variable.get_shape().as_list()).prod()\n",
    "\n",
    "print(\"Total number of trainable parameters: {}\".format(total_num_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-369cf4ed70bf>:5: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n",
      "model/encoder/densenet161/logits/biases is in pretrained model but not in current training model\n",
      "model/encoder/densenet161/logits/weights is in pretrained model but not in current training model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./models/densenet161_imagenet/model\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "coordinator = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coordinator)\n",
    "\n",
    "if pretrained_model != '':\n",
    "    vars_to_restore = get_tensors_in_checkpoint_file(file_name=pretrained_model)\n",
    "    tensors_to_load = build_tensors_in_checkpoint_file(vars_to_restore)\n",
    "    loader = tf.train.Saver(tensors_to_load)\n",
    "    loader.restore(sess, pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0/100, lr: 0.000100000005, loss: 4.742500305176\n",
      "step: 1/100, lr: 0.000099189594, loss: 7.766571044922\n",
      "step: 2/100, lr: 0.000098378368, loss: 2.032992362976\n",
      "step: 3/100, lr: 0.000097566313, loss: 2.815767288208\n",
      "step: 4/100, lr: 0.000096753429, loss: 3.986785173416\n",
      "step: 5/100, lr: 0.000095939686, loss: 4.646549224854\n",
      "step: 6/100, lr: 0.000095125099, loss: 3.015668392181\n",
      "step: 7/100, lr: 0.000094309624, loss: 6.179144382477\n",
      "step: 8/100, lr: 0.000093493290, loss: 4.419931888580\n",
      "step: 9/100, lr: 0.000092676069, loss: 3.328349113464\n",
      "step: 10/100, lr: 0.000091857932, loss: 2.569943904877\n",
      "step: 11/100, lr: 0.000091038892, loss: 4.058022975922\n",
      "step: 12/100, lr: 0.000090218949, loss: 4.076035022736\n",
      "step: 13/100, lr: 0.000089398047, loss: 2.677053213120\n",
      "step: 14/100, lr: 0.000088576213, loss: 2.092865705490\n",
      "step: 15/100, lr: 0.000087753433, loss: 3.993928432465\n",
      "step: 16/100, lr: 0.000086929678, loss: 1.601414084435\n",
      "step: 17/100, lr: 0.000086104934, loss: 3.369278669357\n",
      "step: 18/100, lr: 0.000085279200, loss: 2.692875623703\n",
      "step: 19/100, lr: 0.000084452462, loss: 2.896902084351\n",
      "step: 20/100, lr: 0.000083624691, loss: 2.313728570938\n",
      "step: 21/100, lr: 0.000082795887, loss: 2.706986665726\n",
      "step: 22/100, lr: 0.000081966049, loss: 5.020193099976\n",
      "step: 23/100, lr: 0.000081135135, loss: 2.101335287094\n",
      "step: 24/100, lr: 0.000080303143, loss: 5.419675827026\n",
      "step: 25/100, lr: 0.000079470046, loss: 3.096094608307\n",
      "step: 26/100, lr: 0.000078635858, loss: 3.577145338058\n",
      "step: 27/100, lr: 0.000077800534, loss: 4.712445259094\n",
      "step: 28/100, lr: 0.000076964061, loss: 3.552978992462\n",
      "step: 29/100, lr: 0.000076126424, loss: 3.021361827850\n",
      "step: 30/100, lr: 0.000075287608, loss: 2.213356971741\n",
      "step: 31/100, lr: 0.000074447598, loss: 4.691674709320\n",
      "step: 32/100, lr: 0.000073606367, loss: 3.836879491806\n",
      "step: 33/100, lr: 0.000072763884, loss: 2.044279813766\n",
      "step: 34/100, lr: 0.000071920149, loss: 6.983836174011\n",
      "step: 35/100, lr: 0.000071075148, loss: 2.034986495972\n",
      "step: 36/100, lr: 0.000070228838, loss: 2.281406641006\n",
      "step: 37/100, lr: 0.000069381204, loss: 4.977343559265\n",
      "step: 38/100, lr: 0.000068532230, loss: 2.555978536606\n",
      "step: 39/100, lr: 0.000067681874, loss: 1.869656443596\n",
      "step: 40/100, lr: 0.000066830129, loss: 2.697871208191\n",
      "step: 41/100, lr: 0.000065976958, loss: 3.055565118790\n",
      "step: 42/100, lr: 0.000065122353, loss: 2.851421356201\n",
      "step: 43/100, lr: 0.000064266264, loss: 3.124839305878\n",
      "step: 44/100, lr: 0.000063408668, loss: 4.694957256317\n",
      "step: 45/100, lr: 0.000062549538, loss: 1.816473126411\n",
      "step: 46/100, lr: 0.000061688857, loss: 2.090861558914\n",
      "step: 47/100, lr: 0.000060826569, loss: 3.801974296570\n",
      "step: 48/100, lr: 0.000059962655, loss: 1.758851051331\n",
      "step: 49/100, lr: 0.000059097081, loss: 3.020336389542\n",
      "step: 50/100, lr: 0.000058229805, loss: 2.103170394897\n",
      "step: 51/100, lr: 0.000057360794, loss: 3.951414823532\n",
      "step: 52/100, lr: 0.000056490011, loss: 3.873498916626\n",
      "step: 53/100, lr: 0.000055617409, loss: 3.821273326874\n",
      "step: 54/100, lr: 0.000054742944, loss: 2.432840108871\n",
      "step: 55/100, lr: 0.000053866577, loss: 2.641770362854\n",
      "step: 56/100, lr: 0.000052988267, loss: 2.156643390656\n",
      "step: 57/100, lr: 0.000052107953, loss: 1.142162322998\n",
      "step: 58/100, lr: 0.000051225590, loss: 3.864391326904\n",
      "step: 59/100, lr: 0.000050341128, loss: 3.415584087372\n",
      "step: 60/100, lr: 0.000049454502, loss: 5.802807331085\n",
      "step: 61/100, lr: 0.000048565649, loss: 4.174336433411\n",
      "step: 62/100, lr: 0.000047674523, loss: 3.712228536606\n",
      "step: 63/100, lr: 0.000046781042, loss: 2.715653896332\n",
      "step: 64/100, lr: 0.000045885154, loss: 4.315545082092\n",
      "step: 65/100, lr: 0.000044986766, loss: 5.216360092163\n",
      "step: 66/100, lr: 0.000044085806, loss: 2.760033607483\n",
      "step: 67/100, lr: 0.000043182194, loss: 2.436995029449\n",
      "step: 68/100, lr: 0.000042275835, loss: 2.819437742233\n",
      "step: 69/100, lr: 0.000041366642, loss: 1.972737550735\n",
      "step: 70/100, lr: 0.000040454514, loss: 2.812474012375\n",
      "step: 71/100, lr: 0.000039539336, loss: 2.438351869583\n",
      "step: 72/100, lr: 0.000038621001, loss: 4.211274623871\n",
      "step: 73/100, lr: 0.000037699378, loss: 2.520756959915\n",
      "step: 74/100, lr: 0.000036774327, loss: 3.476078748703\n",
      "step: 75/100, lr: 0.000035845711, loss: 1.804760932922\n",
      "step: 76/100, lr: 0.000034913384, loss: 5.743180274963\n",
      "step: 77/100, lr: 0.000033977150, loss: 3.179786682129\n",
      "step: 78/100, lr: 0.000033036842, loss: 2.528390407562\n",
      "step: 79/100, lr: 0.000032092248, loss: 2.696259260178\n",
      "step: 80/100, lr: 0.000031143147, loss: 2.105888843536\n",
      "step: 81/100, lr: 0.000030189276, loss: 1.880948662758\n",
      "step: 82/100, lr: 0.000029230376, loss: 3.893067836761\n",
      "step: 83/100, lr: 0.000028266133, loss: 1.901784181595\n",
      "step: 84/100, lr: 0.000027296195, loss: 6.242764472961\n",
      "step: 85/100, lr: 0.000026320171, loss: 2.671356678009\n",
      "step: 86/100, lr: 0.000025337617, loss: 3.512465953827\n",
      "step: 87/100, lr: 0.000024348006, loss: 1.518515229225\n",
      "step: 88/100, lr: 0.000023350749, loss: 2.513282060623\n",
      "step: 89/100, lr: 0.000022345139, loss: 2.478606224060\n",
      "step: 90/100, lr: 0.000021330332, loss: 2.767215013504\n",
      "step: 91/100, lr: 0.000020305306, loss: 5.842889785767\n",
      "step: 92/100, lr: 0.000019268804, loss: 2.304722070694\n",
      "step: 93/100, lr: 0.000018219222, loss: 2.812795639038\n",
      "step: 94/100, lr: 0.000017154489, loss: 2.366484403610\n",
      "step: 95/100, lr: 0.000016071775, loss: 2.837803125381\n",
      "step: 96/100, lr: 0.000014967029, loss: 5.307725906372\n",
      "step: 97/100, lr: 0.000013833999, loss: 1.761758565903\n",
      "step: 98/100, lr: 0.000012661769, loss: 3.056997776031\n",
      "step: 99/100, lr: 0.000011426410, loss: 3.840035915375\n"
     ]
    }
   ],
   "source": [
    "start_step = global_step.eval(session=sess)\n",
    "start_time = time.time()\n",
    "duration = 0\n",
    "should_init_iter_op = False\n",
    "if mode == 'train':\n",
    "    should_init_iter_op = True\n",
    "for step in range(start_step, num_total_steps):\n",
    "    before_op_time = time.time()\n",
    "    if step % steps_per_epoch == 0 or should_init_iter_op is True:\n",
    "        sess.run(iter_init_op)\n",
    "        should_init_iter_op = False\n",
    "\n",
    "    _, lr, loss_value = sess.run([apply_gradient_op, learning_rate, total_loss])\n",
    "\n",
    "    print('step: {}/{}, lr: {:.12f}, loss: {:.12f}'.format(step, num_total_steps, lr, loss_value))\n",
    "\n",
    "    duration += time.time() - before_op_time\n",
    "    if step and step % 100 == 0:\n",
    "        examples_per_sec = params.batch_size / duration * 100\n",
    "        duration = 0\n",
    "        time_sofar = (time.time() - start_time) / 3600\n",
    "        training_time_left = (num_total_steps / step - 1.0) * time_sofar\n",
    "        print('%s:' % model_name)\n",
    "        print_string = 'examples/s: {:4.2f} | loss: {:.5f} | time elapsed: {:.2f}h | time left: {:.2f}h'\n",
    "        print(print_string.format(examples_per_sec, loss_value, time_sofar, training_time_left))\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, global_step=step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "    if step and step % 500 == 0:\n",
    "        train_saver.save(sess, log_directory + '/' + model_name + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts_nyu_test training finished\n",
      "2022-05-23 06:10:01.086563\n"
     ]
    }
   ],
   "source": [
    "train_saver.save(sess, log_directory + '/' + model_name + '/model', global_step=num_total_steps)\n",
    "print('%s training finished' % model_name)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
